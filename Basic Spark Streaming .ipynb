{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(time, rdd):\n",
    "    try:\n",
    "        rdd.toDF().registerTempTable(\"records\")\n",
    "    \n",
    "        #Solution 1\n",
    "        query1 = sqlContext.sql('Select Region, Sum(SumOfSumVolume) as totalTrafficVolume \\\n",
    "                            FROM records\\\n",
    "                            GROUP BY Region')\n",
    "    \n",
    "    \n",
    "        query1.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=\"query1\", keyspace=\"streaming\")\\\n",
    "        .save(mode=\"append\")\n",
    "    \n",
    "    \n",
    "        temp = sqlContext.sql('SELECT Region, Site, Sum(SumOfSumVolume) as totalTrafficVolume\\\n",
    "                             FROM records\\\n",
    "                             GROUP BY Region, Site')\n",
    "        temp.registerTempTable(\"temp\")\n",
    "    \n",
    "        temp2 = sqlContext.sql('SELECT records.*, temp.totalTrafficVolume\\\n",
    "                                FROM records\\\n",
    "                                LEFT JOIN temp ON records.Region = temp.Region AND records.Site = temp.Site')\n",
    "    \n",
    "        temp2.registerTempTable(\"temp2\")\n",
    "    \n",
    "    \n",
    "        #Solution 2\n",
    "\n",
    "        query2 = sqlContext.sql(\"SELECT Region, Site, totalTrafficVolume FROM (\\\n",
    "                            SELECT Region, Site, totalTrafficVolume, row_number() over (PARTITION BY Region, Site ORDER BY totalTrafficVolume) as volume_rank\\\n",
    "                            FROM temp2) ranks\\\n",
    "                            GROUP BY Region, Site, totalTrafficVolume\\\n",
    "                            ORDER BY Region, Site, totalTrafficVolume\")\n",
    "    \n",
    "        query2.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=\"query2\", keyspace=\"streaming\")\\\n",
    "        .save(mode=\"append\")\n",
    "\n",
    "    \n",
    "        #Solution 3\n",
    "    \n",
    "        #3a Display the total volume of each sites whose name starts with A\n",
    "        query3a = sqlContext.sql(\"SELECT Site_Description_Cap as Site, Sum(SumOfSumVolume) as totalTrafficVolume\\\n",
    "                            FROM records\\\n",
    "                            GROUP BY Site_Description_Cap\\\n",
    "                            HAVING Site_Description_Cap LIKE 'A%';\")\n",
    "        \n",
    "        query3a.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=\"query3a\", keyspace=\"streaming\")\\\n",
    "        .save(mode=\"append\")\n",
    "       \n",
    "    \n",
    "    \n",
    "        #3b Display the volume in each site in the month of January\n",
    "        query3b = sqlContext.sql(\"SELECT Site, Month, SUM(SumOfSumVolume) as totalTrafficVolume\\\n",
    "                            FROM records\\\n",
    "                            GROUP BY Site, Month\\\n",
    "                            HAVING Month=1\\\n",
    "                            ORDER BY Site\")\n",
    "        query3b.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=\"query3b\", keyspace=\"streaming\")\\\n",
    "        .save(mode=\"append\")\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = (\"Year\", \"Month\", \"Day\", \"Site\", \"SumOfSumVolume\", \"Site_Description_Cap\", \"Region\")\n",
    "Record = namedtuple('Record', fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stream = ssc.textFileStream('file:///home/bdm/Assignment 3/Streaming/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = text_stream.window(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = lines.filter(lambda value: bool(value) != False)\\\n",
    "              .map(lambda value: value.split(\"|\"))\\\n",
    "              .filter(lambda rec: rec[6] != '')\\\n",
    "              .map(lambda rec: Record(rec[0], rec[1], rec[2], rec[3], rec[4], rec[5], rec[6]))\n",
    "record.foreachRDD(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
